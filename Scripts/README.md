# Portfolio RAG System - Data Processing Script

A simple data processing script that reads latest data from GitHub, LinkedIn, and Resume, chunks it, creates embeddings, and writes to MongoDB vector database.

## ğŸ¯ **Single Purpose**

This Scripts folder has **ONE JOB**:
1. **Read** latest data from GitHub, LinkedIn, and Resume
2. **Chunk** the data with sliding window strategy  
3. **Create** embeddings using Google Gemini
4. **Write** everything to MongoDB vector database

That's it. No server, no API, just data processing.

## ğŸ—ï¸ **Simple Structure**

```
Scripts/
â”œâ”€â”€ main.py                    # THE ONLY SCRIPT YOU NEED
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .env                       # Environment variables (create this)
â”œâ”€â”€ final_data.json           # Generated final data (backup)
â”œâ”€â”€ resources/                 # Static resources
â”‚   â””â”€â”€ Resume.pdf            # Your resume file
â”œâ”€â”€ linkedin/                  # LinkedIn scraping
â”‚   â””â”€â”€ linkedin_scraper.py
â”œâ”€â”€ github/                    # GitHub scraping
â”‚   â””â”€â”€ github_scraper.py
â”œâ”€â”€ resume/                    # Resume parsing
â”‚   â””â”€â”€ resume_parser.py
â””â”€â”€ chunking/                  # Text chunking
    â”œâ”€â”€ text_chunker.py
    â””â”€â”€ chunking_config.py
```

## ğŸš€ **How to Use**

### 1. **Setup Environment**

```bash
# Navigate to Scripts directory
cd Scripts

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. **Create .env File**

Create `.env` file in the Scripts directory:

```env
# Google Gemini API
GOOGLE_API_KEY=your_gemini_api_key_here

# MongoDB Atlas Configuration
MONGO_USERNAME=your_mongodb_username
MONGO_PASSWORD=your_mongodb_password
MONGO_APP_NAME=your_app_name
MONGO_DB_NAME=detail-extractor
MONGO_CL_NAME=detail-extractor-collection
MONGO_INDEX_NAME=vector_index_3
MONGO_EMBEDDING_FIELD_NAME=embedding

# User Profile Data
USER_NAME=your_full_name
USER_EMAIL=your_email@example.com
GITHUB_USERNAME=your_github_username
LINKEDIN_URL=https://www.linkedin.com/in/your-profile-url/

# Optional (for scraping)
LINKEDIN_EMAIL=your_linkedin_email@example.com
LINKEDIN_PASSWORD=your_linkedin_password
GITHUB_ACCESS_TOKEN=your_github_access_token
```

### 3. **Add Your Resume**

```bash
# Place your resume in the resources directory
cp /path/to/your/resume.pdf resources/Resume.pdf
```

### 4. **Run the Script**

```bash
# That's it! Just run main.py
python main.py
```

## ğŸ“Š **What main.py Does**

```
ğŸš€ Portfolio RAG Data Processing Script
==================================================
Job: Read data â†’ Chunk â†’ Embed â†’ Store in vector DB
==================================================

ğŸ“– Step 1: Reading latest data from sources...
   ğŸ“‚ Fetching latest GitHub repositories...
   ğŸ”— Scraping latest LinkedIn profile...
   ğŸ“„ Parsing resume...

ğŸ¤– Step 2: Formatting data with Gemini AI...
   ğŸ“ Formatting resume data...
   ğŸ”— Formatting LinkedIn data...
   ğŸ“‚ Formatting GitHub data...

ğŸ”ª Step 4: Setting up chunking system...
   ğŸ“ Chunk size: 512 tokens
   ğŸ”„ Overlap size: 50 tokens

âœ‚ï¸ Step 5: Chunking data with sliding window...
   ğŸ“„ Created X resume chunks
   ğŸ”— Created Y LinkedIn chunks
   ğŸ“‚ Created Z GitHub chunks

ğŸ”® Step 6: Creating embeddings for chunks...
   ğŸ“ Creating embedding for resume chunk 1/X...
   ğŸ”— Creating embedding for LinkedIn chunk 1/Y...
   ğŸ“‚ Creating embedding for GitHub chunk 1/Z...

ğŸ’¾ Step 7: Writing chunks to MongoDB vector database...
   âœ… Written N chunks to MongoDB vector database
   ğŸ“Š Database: detail-extractor
   ğŸ“ Collection: detail-extractor-collection

ğŸ‰ Data processing completed successfully!
âœ… Script completed - Vector database is ready for your chatbot!
```

## ğŸ“¦ **Folder Structure**

### **ğŸ”— linkedin/**
- **File**: `linkedin_scraper.py`
- **Purpose**: Scrape latest LinkedIn profile data

### **ğŸ“‚ github/**
- **File**: `github_scraper.py`
- **Purpose**: Fetch latest GitHub repository data

### **ğŸ“„ resume/**
- **File**: `resume_parser.py`
- **Purpose**: Parse resume PDF and extract text

### **ğŸ”ª chunking/**
- **Files**: 
  - `text_chunker.py` - SlidingWindowChunker class
  - `chunking_config.py` - Configuration settings
- **Purpose**: Chunk text with overlapping windows and metadata

## ğŸ”§ **Configuration**

### **Chunking Parameters**
```python
# In chunking/chunking_config.py
CHUNKING_CONFIG = {
    "chunk_size": 512,           # Maximum tokens per chunk
    "overlap_size": 50,          # Tokens to overlap between chunks
    "max_chunks_per_query": 5,   # Maximum chunks to retrieve per query
    "embedding_delay": 1,        # API rate limiting (seconds)
}
```

## ğŸ¯ **When to Run**

Run this script whenever you want to update your vector database with:

- **New GitHub repositories** or updated descriptions
- **Updated LinkedIn profile** information
- **Updated resume** with new experiences/skills
- **After making changes** to any of your data sources

## ğŸ“ˆ **Performance Features**

### **Sliding Window Chunking**
- **Context Preservation**: Overlapping chunks maintain context
- **Precise Retrieval**: Specific sections instead of entire documents
- **Token Efficiency**: Optimized chunk sizes for LLM processing
- **Metadata Enhancement**: Section-aware retrieval

### **Vector Search Ready**
- **Semantic Similarity**: Google Gemini embeddings
- **MongoDB Atlas**: Vector search index
- **Multi-source**: Resume, LinkedIn, GitHub data
- **Real-time Ready**: Fast retrieval for chatbot responses

## ğŸ”’ **Security**

- **Environment Variables**: Sensitive data in `.env` file
- **API Key Management**: Secure Google API key handling
- **Rate Limiting**: API call throttling to avoid limits

## ğŸ› ï¸ **Development**

### **Adding New Data Sources**
1. Create new folder (e.g., `twitter/`, `medium/`)
2. Add scraper file
3. Import and use in `main.py`

### **Modifying Chunking**
1. Edit `chunking/chunking_config.py`
2. Adjust parameters as needed

## ğŸ“Š **Output**

The script creates:
- **MongoDB Vector Database**: All chunks with embeddings stored
- **final_data.json**: Backup of processed data
- **Console Output**: Detailed progress and summary

## ğŸš€ **Integration**

Once you run this script, your MongoDB vector database is ready to be used by:
- Your portfolio website chatbot
- Any RAG application
- Vector search queries
- AI-powered resume matching

---

**ğŸ‰ Simple, focused, and effective - exactly what you need!**